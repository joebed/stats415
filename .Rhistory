which.min(svm_rad$performances$error)
which.min(train_err)
test_err
best <- svm_rad$best.model
summary(best)
svm_rad$best.parameters
degreevals <- c(.5, 1, 2, 3, 4, 5, 6)
train_err <- matrix(0, 21, length(degreevals))
test_err <- matrix(0, 21, length(degreevals))
for (j in 1:length(degreevals)){
for (i in seq(1,21)){
svm2 <- svm(sp_adj ~ ., data = train, kernel = "polynomial",
degree = degreevals[j],
cost = costvals[i], scale = FALSE)
svm_train_pred <- predict(svm2, train)
svm_test_pred <- predict(svm2, test)
train_err[i,j] <- 1-(mean(svm_train_pred == train$sp_adj))
test_err[i,j] <- 1-(mean(svm_test_pred == test$sp_adj))
}
}
train_err_df <- data.frame(train_err)
train_err_df$idx <- 1:21
test_err_df <- data.frame(test_err)
test_err_df$idx <- 1:21
df1 <- train_err_df
df2 <- test_err_df
df1_reshaped <- data.frame(x=df1$idx,
y=c(df1$X1, df1$X2, df1$X3, df1$X4,
df1$X5, df1$X6, df1$X7),
gamma=c(rep("0.5", nrow(df1)),
rep("1", nrow(df1)),
rep("2", nrow(df1)),
rep("3", nrow(df1)),
rep("4", nrow(df1)),
rep("5", nrow(df1)),
rep("6", nrow(df1))))
df2_reshaped <- data.frame(x=df2$idx,
y=c(df2$X1, df2$X2, df2$X3, df2$X4,
df2$X5, df2$X6, df2$X7),
gamma=c(rep("0.5", nrow(df2)),
rep("1", nrow(df2)),
rep("2", nrow(df2)),
rep("3", nrow(df2)),
rep("4", nrow(df2)),
rep("5", nrow(df2)),
rep("6", nrow(df2))))
ggplot(df1_reshaped, aes(x, y, col=gamma))+geom_line()
ggplot(df1_reshaped, aes(x, y, col=gamma))+geom_line()
ggplot(df2_reshaped, aes(x, y, col=gamma))+geom_line()
train_err
test_err
ggplot(df2_reshaped, aes(x, y, col=gamma))+geom_line()
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
data(crabs)
set.seed(6789)
N <- 50
g1 <- subset(crabs, sp == 'O' & sex == 'F')
g2 <- subset(crabs, sp == 'B' & sex == 'F')
g3 <- subset(crabs, sp == 'O' & sex == 'M')
g4 <- subset(crabs, sp == 'B' & sex == 'M')
g1_sp <- sample(seq(N), floor(.8*N), replace = FALSE)
g2_sp <- sample(seq(N), floor(.8*N), replace = FALSE)
g3_sp <- sample(seq(N), floor(.8*N), replace = FALSE)
g4_sp <- sample(seq(N), floor(.8*N), replace = FALSE)
g1_train <- g1[g1_sp,]
g1_test <- g1[-g1_sp,]
g2_train <- g2[g2_sp,]
g2_test <- g2[-g2_sp,]
g3_train <- g3[g3_sp,]
g3_test <- g3[-g3_sp,]
g4_train <- g4[g4_sp,]
g4_test <- g4[-g4_sp,]
train_old <- rbind(g1_train, g2_train, g3_train, g4_train)
test_old <- rbind(g1_test, g2_test, g3_test, g4_test)
train_old$sp_adj <- as.factor(ifelse(train_old$sp == "B", 1, 0))
test_old$sp_adj <- as.factor(ifelse(test_old$sp == "B", 1, 0))
train <- subset(train_old, select = -c(sex, index, sp))
test <- subset(test_old, select = -c(sex, index, sp))
library(e1071)
costvals <- c(.001,.01,.1,1,2,3,4,5,7,9,11,13,15,20,25,30,35,40,45,50,100)
train_err <- seq(1,21)
test_err <- seq(1,21)
for (i in seq(1,21)){
svm1 <- svm(sp_adj ~ ., data = train, kernel = "linear",
cost = costvals[i],
scale = FALSE)
svm_train_pred <- predict(svm1, train)
svm_test_pred <- predict(svm1, test)
train_err[i] <- 1-(mean(svm_train_pred == train$sp_adj))
test_err[i] <- 1-(mean(svm_test_pred == test$sp_adj))
}
plot(train_err)
which.min(train_err)
plot(test_err)
which.min(test_err)
svm_tuned <- tune(svm, sp_adj ~ ., data = train, kernel="linear", ranges = list(cost = costvals))
plot(svm_tuned$performances$error)
which.min(svm_tuned$performances$error)
svm_tuned$performances$error
gammavals <- c(.5, 1, 2, 3, 4, 5, 6)
train_err <- matrix(0, 21, length(gammavals))
test_err <- matrix(0, 21, length(gammavals))
for (j in 1:length(gammavals)){
for (i in seq(1,21)){
svm1 <- svm(sp_adj ~ ., data = train, kernel = "radial",
gamma = gammavals[j],
cost = costvals[i], scale = FALSE)
svm_train_pred <- predict(svm1, train)
svm_test_pred <- predict(svm1, test)
train_err[i,j] <- 1-(mean(svm_train_pred == train$sp_adj))
test_err[i,j] <- 1-(mean(svm_test_pred == test$sp_adj))
}
}
train_err_df <- data.frame(train_err)
train_err_df$idx <- 1:21
test_err_df <- data.frame(test_err)
test_err_df$idx <- 1:21
library(ggplot2)
df1 <- train_err_df
df2 <- test_err_df
df1_reshaped <- data.frame(x=df1$idx,
y=c(df1$X1, df1$X2, df1$X3, df1$X4,
df1$X5, df1$X6, df1$X7),
gamma=c(rep("0.5", nrow(df1)),
rep("1", nrow(df1)),
rep("2", nrow(df1)),
rep("3", nrow(df1)),
rep("4", nrow(df1)),
rep("5", nrow(df1)),
rep("6", nrow(df1))))
df2_reshaped <- data.frame(x=df2$idx,
y=c(df2$X1, df2$X2, df2$X3, df2$X4,
df2$X5, df2$X6, df2$X7),
gamma=c(rep("0.5", nrow(df2)),
rep("1", nrow(df2)),
rep("2", nrow(df2)),
rep("3", nrow(df2)),
rep("4", nrow(df2)),
rep("5", nrow(df2)),
rep("6", nrow(df2))))
ggplot(df1_reshaped, aes(x, y, col=gamma))+geom_line()
ggplot(df2_reshaped, aes(x, y, col=gamma))+geom_line()
svm_rad <- tune(svm, sp_adj ~ ., data = train,
kernel="radial", ranges=list(cost=c(.001,.01,.1,1,2,3,4,5,7,9,11,13,15,20,25,30,35,40,45,50,100),
gamma=c(.5, 1, 2, 3, 4, 5, 6)))
plot(svm_rad$performances$error)
svm_rad$best.parameters
degreevals <- c(.5, 1, 2, 3, 4, 5, 6)
train_err <- matrix(0, 21, length(degreevals))
test_err <- matrix(0, 21, length(degreevals))
for (j in 1:length(degreevals)){
for (i in seq(1,21)){
svm2 <- svm(sp_adj ~ ., data = train, kernel = "polynomial",
degree = degreevals[j],
cost = costvals[i], scale = FALSE)
svm_train_pred <- predict(svm2, train)
svm_test_pred <- predict(svm2, test)
train_err[i,j] <- 1-(mean(svm_train_pred == train$sp_adj))
test_err[i,j] <- 1-(mean(svm_test_pred == test$sp_adj))
}
}
train_err_df <- data.frame(train_err)
train_err_df$idx <- 1:21
test_err_df <- data.frame(test_err)
test_err_df$idx <- 1:21
df1 <- train_err_df
df2 <- test_err_df
df1_reshaped <- data.frame(x=df1$idx,
y=c(df1$X1, df1$X2, df1$X3, df1$X4,
df1$X5, df1$X6, df1$X7),
gamma=c(rep("0.5", nrow(df1)),
rep("1", nrow(df1)),
rep("2", nrow(df1)),
rep("3", nrow(df1)),
rep("4", nrow(df1)),
rep("5", nrow(df1)),
rep("6", nrow(df1))))
df2_reshaped <- data.frame(x=df2$idx,
y=c(df2$X1, df2$X2, df2$X3, df2$X4,
df2$X5, df2$X6, df2$X7),
gamma=c(rep("0.5", nrow(df2)),
rep("1", nrow(df2)),
rep("2", nrow(df2)),
rep("3", nrow(df2)),
rep("4", nrow(df2)),
rep("5", nrow(df2)),
rep("6", nrow(df2))))
ggplot(df1_reshaped, aes(x, y, col=gamma))+geom_line()
ggplot(df2_reshaped, aes(x, y, col=gamma))+geom_line()
svm_poly <- tune(svm, sp_adj ~ ., data = train,
kernel="polynomial", ranges=list(cost=c(.001,.01,.1,1,2,3,4,5,7,9,11,13,15,20,25,30,35,40,45,50,100),
degree=c(.5, 1, 2, 3, 4, 5, 6)))
plot(svm_rad$performances$error)
svm_rad$best.parameters
svm_poly <- tune(svm, sp_adj ~ ., data = train,
kernel="polynomial", ranges=list(cost=c(.001,.01,.1,1,2,3,4,5,7,9,11,13,15,20,25,30,35,40,45,50,100),
degree=c(.5, 1, 2, 3, 4, 5, 6)))
plot(svm_poly$performances$error)
svm_$best.parameters
svm_poly$best.parameters
test_err
df_plot <- test
svm_plot <- svm(sp_adj ~ ., data = train, kernel = "linear",
cost = 1,
scale = FALSE)
svm_test_pred <- predict(svm_plot, test)
svm_test_pred
df_plot <- test
svm_plot <- svm(sp_adj ~ ., data = train, kernel = "linear",
cost = 1,
scale = FALSE)
svm_test_pred <- predict(svm_plot, test)
df_plot$pred <- svm_test_pred
library(ggplot2)
df_plot <- test
svm_plot <- svm(sp_adj ~ ., data = train, kernel = "linear",
cost = 1,
scale = FALSE)
svm_test_pred <- predict(svm_plot, test)
df_plot$pred <- svm_test_pred
ggplot() + geom_point(data = df_plot, aes(x = FL, y = RW, color = pred))
ggplot() + geom_point(data = df_plot, aes(x = FL, y = RW, color = sp_adj))
df_plot <- test
svm_plot <- svm(sp_adj ~ ., data = train, kernel = "linear",
cost = .001,
scale = FALSE)
svm_test_pred <- predict(svm_plot, test)
df_plot$pred <- svm_test_pred
ggplot() + geom_point(data = df_plot, aes(x = FL, y = RW, color = pred))
ggplot() + geom_point(data = df_plot, aes(x = FL, y = RW, color = sp_adj))
ggplot() + geom_point(data = df_plot, aes(x = FL, y = RW, color = pred)) + theme_minimal()
ggplot() + geom_point(data = df_plot, aes(x = FL, y = RW, color = sp_adj)) + theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(MASS)
data(crabs)
set.seed(6789)
N <- 50
g1 <- subset(crabs, sp == 'O' & sex == 'F')
g2 <- subset(crabs, sp == 'B' & sex == 'F')
g3 <- subset(crabs, sp == 'O' & sex == 'M')
g4 <- subset(crabs, sp == 'B' & sex == 'M')
g1_sp <- sample(seq(N), floor(.8*N), replace = FALSE)
g2_sp <- sample(seq(N), floor(.8*N), replace = FALSE)
g3_sp <- sample(seq(N), floor(.8*N), replace = FALSE)
g4_sp <- sample(seq(N), floor(.8*N), replace = FALSE)
g1_train <- g1[g1_sp,]
g1_test <- g1[-g1_sp,]
g2_train <- g2[g2_sp,]
g2_test <- g2[-g2_sp,]
g3_train <- g3[g3_sp,]
g3_test <- g3[-g3_sp,]
g4_train <- g4[g4_sp,]
g4_test <- g4[-g4_sp,]
train_old <- rbind(g1_train, g2_train, g3_train, g4_train)
test_old <- rbind(g1_test, g2_test, g3_test, g4_test)
train_old$sp_adj <- as.factor(ifelse(train_old$sp == "B", 1, 0))
test_old$sp_adj <- as.factor(ifelse(test_old$sp == "B", 1, 0))
train <- subset(train_old, select = -c(sex, index, sp))
test <- subset(test_old, select = -c(sex, index, sp))
library(e1071)
costvals <- c(.001,.01,.1,1,2,3,4,5,7,9,11,13,15,20,25,30,35,40,45,50,100)
train_err <- seq(1,21)
test_err <- seq(1,21)
for (i in seq(1,21)){
svm1 <- svm(sp_adj ~ ., data = train, kernel = "linear",
cost = costvals[i],
scale = FALSE)
svm_train_pred <- predict(svm1, train)
svm_test_pred <- predict(svm1, test)
train_err[i] <- 1-(mean(svm_train_pred == train$sp_adj))
test_err[i] <- 1-(mean(svm_test_pred == test$sp_adj))
}
plot(train_err)
which.min(train_err)
plot(test_err)
which.min(test_err)
svm_tuned <- tune(svm, sp_adj ~ ., data = train, kernel="linear", ranges = list(cost = costvals))
plot(svm_tuned$performances$error)
which.min(svm_tuned$performances$error)
svm_tuned$performances$error
df_plot <- test
svm_plot <- svm(sp_adj ~ ., data = train, kernel = "linear",
cost = .001,
scale = FALSE)
svm_test_pred <- predict(svm_plot, test)
df_plot$pred <- svm_test_pred
ggplot() + geom_point(data = df_plot, aes(x = FL, y = RW, color = pred)) + theme_minimal()
ggplot() + geom_point(data = df_plot, aes(x = FL, y = RW, color = sp_adj)) + theme_minimal()
gammavals <- c(.5, 1, 2, 3, 4, 5, 6)
train_err <- matrix(0, 21, length(gammavals))
test_err <- matrix(0, 21, length(gammavals))
for (j in 1:length(gammavals)){
for (i in seq(1,21)){
svm1 <- svm(sp_adj ~ ., data = train, kernel = "radial",
gamma = gammavals[j],
cost = costvals[i], scale = FALSE)
svm_train_pred <- predict(svm1, train)
svm_test_pred <- predict(svm1, test)
train_err[i,j] <- 1-(mean(svm_train_pred == train$sp_adj))
test_err[i,j] <- 1-(mean(svm_test_pred == test$sp_adj))
}
}
train_err_df <- data.frame(train_err)
train_err_df$idx <- 1:21
test_err_df <- data.frame(test_err)
test_err_df$idx <- 1:21
df1 <- train_err_df
df2 <- test_err_df
df1_reshaped <- data.frame(x=df1$idx,
y=c(df1$X1, df1$X2, df1$X3, df1$X4,
df1$X5, df1$X6, df1$X7),
gamma=c(rep("0.5", nrow(df1)),
rep("1", nrow(df1)),
rep("2", nrow(df1)),
rep("3", nrow(df1)),
rep("4", nrow(df1)),
rep("5", nrow(df1)),
rep("6", nrow(df1))))
df2_reshaped <- data.frame(x=df2$idx,
y=c(df2$X1, df2$X2, df2$X3, df2$X4,
df2$X5, df2$X6, df2$X7),
gamma=c(rep("0.5", nrow(df2)),
rep("1", nrow(df2)),
rep("2", nrow(df2)),
rep("3", nrow(df2)),
rep("4", nrow(df2)),
rep("5", nrow(df2)),
rep("6", nrow(df2))))
ggplot(df1_reshaped, aes(x, y, col=gamma))+geom_line()
ggplot(df2_reshaped, aes(x, y, col=gamma))+geom_line()
svm_rad <- tune(svm, sp_adj ~ ., data = train,
kernel="radial", ranges=list(cost=c(.001,.01,.1,1,2,3,4,5,7,9,11,13,15,20,25,30,35,40,45,50,100),
gamma=c(.5, 1, 2, 3, 4, 5, 6)))
plot(svm_rad$performances$error)
svm_rad$best.parameters
test_err
degreevals <- c(.5, 1, 2, 3, 4, 5, 6)
train_err <- matrix(0, 21, length(degreevals))
test_err <- matrix(0, 21, length(degreevals))
for (j in 1:length(degreevals)){
for (i in seq(1,21)){
svm2 <- svm(sp_adj ~ ., data = train, kernel = "polynomial",
degree = degreevals[j],
cost = costvals[i], scale = FALSE)
svm_train_pred <- predict(svm2, train)
svm_test_pred <- predict(svm2, test)
train_err[i,j] <- 1-(mean(svm_train_pred == train$sp_adj))
test_err[i,j] <- 1-(mean(svm_test_pred == test$sp_adj))
}
}
train_err_df <- data.frame(train_err)
train_err_df$idx <- 1:21
test_err_df <- data.frame(test_err)
test_err_df$idx <- 1:21
df1 <- train_err_df
df2 <- test_err_df
df1_reshaped <- data.frame(x=df1$idx,
y=c(df1$X1, df1$X2, df1$X3, df1$X4,
df1$X5, df1$X6, df1$X7),
gamma=c(rep("0.5", nrow(df1)),
rep("1", nrow(df1)),
rep("2", nrow(df1)),
rep("3", nrow(df1)),
rep("4", nrow(df1)),
rep("5", nrow(df1)),
rep("6", nrow(df1))))
df2_reshaped <- data.frame(x=df2$idx,
y=c(df2$X1, df2$X2, df2$X3, df2$X4,
df2$X5, df2$X6, df2$X7),
gamma=c(rep("0.5", nrow(df2)),
rep("1", nrow(df2)),
rep("2", nrow(df2)),
rep("3", nrow(df2)),
rep("4", nrow(df2)),
rep("5", nrow(df2)),
rep("6", nrow(df2))))
ggplot(df1_reshaped, aes(x, y, col=gamma))+geom_line()
ggplot(df2_reshaped, aes(x, y, col=gamma))+geom_line()
svm_poly <- tune(svm, sp_adj ~ ., data = train,
kernel="polynomial", ranges=list(cost=c(.001,.01,.1,1,2,3,4,5,7,9,11,13,15,20,25,30,35,40,45,50,100),
degree=c(.5, 1, 2, 3, 4, 5, 6)))
plot(svm_poly$performances$error)
svm_poly$best.parameters
knitr::opts_chunk$set(echo = TRUE)
## first diastolic reading
hist(train_data$BPXDI1)
read.csv("train_data")
read.csv(train_data)
read.csv("train_data")
source("~/stats415/set_workspace.R", echo=TRUE)
lm.fit = lm(y ~ ., train_data)
head(train_data)
View(test_data)
train_data(,-SMAQUEX2)
lm.fit <- lm(LBXTC ~ DR1TCHOL, train_data)
summary(lm.fit)
fit <- lm(LBXTC ~ poly(DR1TCHOL, 5), data = train_data)
summary(fit)
fit <- lm(LBXTC ~ poly(DR1TCHOL, 10), data = train_data)
> summary(fit)
fit <- lm(LBXTC ~ poly(DR1TCHOL, 10), data = train_data)
summary(fit)
plot(lm.fit)
plot(lm.fit)
fit <- lm(LBXTC ~ poly(DR1TCHOL, 4), data = train_data)
summary(fit)
plot(lm.fit)
plot(lm.fit)
plot(fit)
fit <- lm(LBXTC ~ log(DR1TCHOL), data = train_data)
unnecessary <- c(SMAQUEX2, LBDTCSI, WTDRD1, WTDR2D, DR1EXMER,
DRDINT, DR1DBIH, DR1LANG, DR1NUMF, DR1DAY,
DBQ095Z, DRQSPREP, DRQSDIET)
unnecessary <- c(train_data$SMAQUEX2, train_data$LBDTCSI, train_data$WTDRD1, train_data$WTDR2D, train_data$DR1EXMER, train_data$DRDINT, train_data$DR1DBIH, train_data$DR1LANG, DR1NUMF, DR1DAY,
DBQ095Z, DRQSPREP, DRQSDIET)
unnecessary <- c(train_data$SMAQUEX2, train_data$LBDTCSI, train_data$WTDRD1, train_data$WTDR2D, train_data$DR1EXMER, train_data$DRDINT, train_data$DR1DBIH, train_data$DR1LANG, train_data$DR1NUMF, train_data$DR1DAY, train_data$DBQ095Z, train_data$DRQSPREP, train_data$DRQSDIET)
fit <- lm(LBXTC ~ . -unnecessary, train_data)
all_var <- subset(train_data, select = -c(SMAQUEX2, LBDTCSI, WTDRD1, WTDR2D, DR1EXMER, DRDINT, DR1DBIH, DR1LANG, DR1NUMF, DR1DAY, DBQ095Z, DRQSPREP, DRQSDIET))
all_var <- subset(train_data, select = -c(SMAQUEX2, LBDTCSI, WTDRD1, WTDR2D, DR1EXMER, DRDINT, DR1DBIH, DR1LANG, DR1TNUMF, DR1DAY, DBQ095Z, DRQSPREP, DRQSDIET))
fit <- lm(LBXTC ~ ., all_var)
summary(fit)
all_var <- subset(train_data, select = -c(y, SEQN, SMAQUEX2, LBDTCSI, WTDRD1, WTDR2D, DR1EXMER, DRDINT, DR1DBIH, DR1LANG, DR1TNUMF, DR1DAY, DBQ095Z, DRQSPREP, DRQSDIET))
library(leaps)
regfit.full <- regsubsets(LBXTC ~ ., all_var)
summary(regfit.full)
regfit.full <- regsubsets(LBXTC ~ ., all_var)
View(all_var)
View(train_data)
source('~/R/Stats_415/Stats_415_Final_Project/set_workspace.R', echo=TRUE)
setwd("~/R/Stats_415/Stats_415_Final_Project")
train_data = read.csv("train_data.csv")
test_data = read.csv("test_data.csv")
train_data = train_data[,seq(2, 145)]
test_data = test_data[,seq(2, 145)]
rm(wd)
str(train_data)
head(train_data)
summary(train_data$SMAQUEX2)
table(train_data$SMAQUEX2)
table(train_data$SMAQUEX2)
LBXTC
summary(train_data$LBXTC)
str(train_data)
head(train_data)
train_data$BP_Systolic = mean(train_data$BPXSY1, train_data$BPXSY2, train_data$BPXSY3)
train_data$BP_Systolic = (train_data$BPXSY1 + train_data$BPXSY2 + train_data$BPXSY3)/3
summary(train_data$BP_Systolic)
train_data$BP_Systolic
summary(train_data$BPXSY1)
summary(train_data$BP_Systolic)
train_data$BP_Systolic = (train_data$BPXDI1 + train_data$BPXDI2 + train_data$BPXDI3)/3
train_data$BP_Systolic = (train_data$BPXSY1 + train_data$BPXSY2 + train_data$BPXSY3)/3
train_data$BP_Diastolic = (train_data$BPXDI1 + train_data$BPXDI2 + train_data$BPXDI3)/3
train_data$BP_Diastolic = (train_data$BPXDI1 + train_data$BPXDI2 + train_data$BPXDI3)/3
summary(train_data$BP_Diastolic)
summary(train_data$LBXTC)
summary(train_data$BMXBMI)
## Decision Tree with Random Forests
table(train_data$SMAQUEX2)
train_data$smokes = as.factor(ifelse(train_data$SMAQUEX2 < 0, "No", "Yes")
train_data$smokes = as.factor(ifelse(train_data$SMAQUEX2 < 0, "No", "Yes"))
train_data$smokes = as.factor(ifelse(train_data$SMAQUEX2 < 0, "No", "Yes"))
summary(train_data$smokes)
train_data$smokes = NULL
## Decision Tree with Boosting
cholest = read.csv("train_data.csv")
cholest$LBXTC
summary(cholest$LBXTC)
?unscale
source('~/R/Stats_415/Stats_415_Final_Project/data/set_up_data.R', echo=TRUE)
train_data = read.csv("train_data.csv")
source('~/R/Stats_415/Stats_415_Final_Project/data/set_up_data.R', echo=TRUE)
summary(train_data$LBXTC)
train_data$highchol = as.factor(ifelse(train_data$LBXTC >= 239, 1, 0))
train_data$borderlinechol = as.factor(ifelse(train_data$LBXTC >= 200, 1, 0))
test_data$borderlinechol = as.factor(ifelse(test_data$LBXTC >= 200, 1, 0))
test_data$borderlinechol = as.factor(ifelse(test_data$LBXTC >= 200, 1, 0))
train_data = scale(train_data)
test_data = scale(test_data)
train_data[,] = scale(train_data[,])
lm(highchol~!borderlinechol, data = train_data)
#install.packages("randomForest")
library(randomForest)
train_data$y = NULL
#install.packages("randomForest")
library(randomForest)
train_data$LBXTC = NULL
test_data$LBXTC = NULL
m = 12
mod_rf = randomForest(highchol~ .-borderlinechol, data = train_data, mtry = m, ntree = 10000, importance = T)
varImpPlot(mod_rf)
mod_rf = randomForest(highchol~ .-borderlinechol, data = train_data, mtry = m, ntree = 1000, importance = T)
varImpPlot(mod_rf)
train_data$LBDTCSI = NULL
test_data$LBDTCSI = NULL
mod_rf = randomForest(highchol~ .-borderlinechol, data = train_data, mtry = m, ntree = 1000, importance = T)
varImpPlot(mod_rf)
mod_rf_2 = randomForest(borderlinechol~ .-highchol, data = train_data, mtry = m, ntree = 1000, importance = T)
train_data_b$highchol = ifelse(train_data_b$highchol == "Yes", 1, 0)
test_data_b = test_data
test_data_b$highchol = ifelse(test_data_b$highchol == "Yes", 1, 0)
## Decision Tree with Boosting
train_data_b = train_data
test_data_b$highchol
test_data_b = test_data
test_data_b$highchol
head(test_data_b)
test_data$highchol = as.factor(ifelse(test_data$LBXTC >= 239, 1, 0))
source('~/R/Stats_415/Stats_415_Final_Project/data/set_up_data.R', echo=TRUE)
source('~/R/Stats_415/Stats_415_Final_Project/Decision_Trees_ANN.R', echo=TRUE)
