preds = predict(polyfit, newdata = list(indus = Boston$indus[-train_idx]), se = TRUE)
testData <- Boston[-train_idx, ]
## polynomial test RSS
mean((preds$nox-testData$nox)^2)
## Optimal Degrees of Freedom
indusgrid <- seq(from = min(Boston$indus), to = max(Boston$indus), by = 0.5)
preds <- predict(polyfit, newdata = data.frame(indus = indusgrid))
plot(Boston$nox ~ Boston$indus, cex = .5, col = "grey",
xlab = "indus", ylab = "nox")
lines(indusgrid, preds, lwd = 2)
## One less degree of freedom
polyfitless <- lm(nox ~ poly(indus, 8), data = Boston[train_idx,])
coef(summary(polyfitless))
summary(polyfitless)$r.squared
predsless <- predict(polyfitless, newdata = data.frame(indus = indusgrid))
plot(Boston$nox ~ Boston$indus, cex = .5, col = "grey",
xlab = "indus", ylab = "nox")
lines(indusgrid, predsless, lwd = 2)
## One more degree of freedom
polyfitmore <- lm(nox ~ poly(indus, 10), data = Boston[train_idx,])
coef(summary(polyfitmore))
summary(polyfitmore)$r.squared
predsmore <- predict(polyfitmore, newdata = data.frame(indus = indusgrid))
plot(Boston$nox ~ Boston$indus, cex = .5, col = "grey",
xlab = "indus", ylab = "nox")
lines(indusgrid, predsmore, lwd = 2)
## It appears that the r-squared for the 10th degree polynomial
## is slightly higher than that of the 9th degree polynomial.
## For all three models, the p value appears to be significant for all coefficients.
## The regression provides and r-squared value of 0.7319199.
## This r-squared value is not super close to 1, but the p-values
## of the coefficients display that they are all statistically significant.
## smooth spline
fit.10 <- smooth.spline(Boston$indus[train_idx], Boston$nox[train_idx],
lambda = 10)
fit.0.1 <- smooth.spline(Boston$indus[train_idx], Boston$nox[train_idx],
lambda = 0.1)
fit.10$df
fit.0.1$df
plot(Boston$indus, Boston$nox, cex = .5, col = "grey")
lines(fit.10, col = "blue", lwd = 2)
lines(fit.0.1, col = "red", lwd = 2)
legend("topright", legend = c("lambda = 10", "lambda = 0.1"),
col = c("blue", "red"), lty = 1)
## smoothing spline test error for lambda = 0.1
mean((predict(fit.0.1, x = Boston$indus[-train_idx])$y - Boston$nox[-train_idx])^2)
## smoothing spline test error for lambda = 10
mean((predict(fit.10, x = Boston$indus[-train_idx])$y - Boston$nox[-train_idx])^2)
## lambda = 0.1 and 10 show larger degrees of freedom
fit.ss <- smooth.spline(Boston$indus[train_idx], Boston$nox[train_idx],
cv = TRUE)
## optimal degrees of freedom
fit.ss$df
fit.ss$lambda
plot(Boston$indus, Boston$nox, cex = .5, col = "grey")
lines(fit.ss, col = "blue", lwd = 2)
## smoothing spline test error
mean((predict(fit.ss, x = Boston$indus[-train_idx])$y - Boston$nox[-train_idx])^2)
## The test error shows that the smoothing spline with the optimal
## degrees of freedom has the lowest test error. It is not very smooth,
## but that makes sense given the spread of the data.
## smaller degrees of freedom
fit.20 <- smooth.spline(Boston$indus[train_idx], Boston$nox[train_idx],
lambda = 20)
## test error
mean((predict(fit.20, x = Boston$indus[-train_idx])$y - Boston$nox[-train_idx])^2)
## natural splines
cvs <- rep(NA, 10)
for (i in 3:10) {
fit <- glm(nox ~ ns(indus, df = i), data = Boston)
cvs[i] <- cv.glm(Boston, fit, K = 10)$delta[1]
}
plot(3:10, cvs[-c(1, 2)], xlab = "Cuts", ylab = "Test MSE", type = "l")
d.min <- which.min(cvs)
points(which.min(cvs), cvs[which.min(cvs)], col = "red", cex = 2, pch = 20)
## It appears that the lowest error occurs
## when the degrees of freedom at 10.
## Optimal Degrees of Freedom
fit <- lm(nox ~ ns(indus, df = 10), data = Boston[train_idx,])
predns <- predict(fit, newdata = data.frame(indus = indusgrid))
plot(Boston$indus, Boston$nox, cex = .5, col = "grey",
xlab = "indus", ylab = "nox")
lines(indusgrid, predns, lwd = 2)
## test error
mean((predns - Boston$nox[-train_idx])^2)
summary(fit)
## For the most part, the p-values show that the coefficients
## are mostly significant. There are a few sections where the
## coefficients are not statistically significant at the 95% level.
## Less Degrees of Freedom
fitless <- lm(nox ~ ns(indus, df = 5), data = Boston[train_idx,])
prednsless <- predict(fitless, newdata = data.frame(indus = indusgrid))
plot(Boston$indus, Boston$nox, cex = .5, col = "grey",
xlab = "indus", ylab = "nox")
lines(indusgrid, prednsless, lwd = 2)
## test error
mean((prednsless - Boston$nox[-train_idx])^2)
## Optimal Degrees of Freedom
fitmore <- lm(nox ~ ns(indus, df = 15), data = Boston[train_idx,])
prednsmore <- predict(fitmore, newdata = data.frame(indus = indusgrid))
plot(Boston$indus, Boston$nox, cex = .5, col = "grey",
xlab = "indus", ylab = "nox")
lines(indusgrid, prednsmore, lwd = 2)
## test error
mean((prednsmore - Boston$nox[-train_idx])^2)
## The test error is the lowest when the degrees of freedom
## are at their optimum value.
library(gam)
gamfit = gam(nox ~ s(indus, 16) + s(dis, 16) + s(rad, 16), data = Boston[-train_idx, ])
summary(gamfit)
par(mfrow = c(1,3))
plot(gamfit)
knitr::opts_chunk$set(echo = TRUE)
library(faraway)
library(MASS)
data(ozone)
summary(ozone)
pairs(~ O3 + temp + humidity + ibh, data = ozone)
hist(O3)
data(ozone)
summary(ozone)
pairs(~ O3 + temp + humidity + ibh, data = ozone)
hist(O3)
data(ozone)
summary(ozone)
pairs(~ O3 + temp + humidity + ibh, data = ozone)
hist(O3)
data(ozone)
summary(ozone)
pairs(~ O3 + temp + humidity + ibh, data = ozone)
hist(O3$ozone)
data(ozone)
summary(ozone)
pairs(~ O3 + temp + humidity + ibh, data = ozone)
hist(ozone$O3)
hist(temp + O3, data = ozone)
data(ozone)
summary(ozone)
pairs(~ O3 + temp + humidity + ibh, data = ozone)
hist(ozone$O3)
hist(ozone$temp + ozone$O3)
data(ozone)
summary(ozone)
par(mfrow = c(3, 1))
pairs(~ O3 + temp + humidity + ibh, data = ozone)
hist(ozone$O3)
hist(ozone$temp + ozone$O3)
data(ozone)
summary(ozone)
par(mfrow = c(1, 3))
pairs(~ O3 + temp + humidity + ibh, data = ozone)
hist(ozone$O3)
hist(ozone$temp + ozone$O3)
data(ozone)
summary(ozone)
par(mfrow = c(1, 3))
pairs(~ O3 + temp + humidity + ibh, data = ozone)
hist(ozone$O3)
hist(ozone$temp + ozone$O3)
data(ozone)
summary(ozone)
par(mfrow = c(1, 2))
pairs(~ O3 + temp + humidity + ibh, data = ozone)
hist(ozone$O3)
hist(ozone$temp + ozone$O3)
data(ozone)
summary(ozone)
par(mfrow = c(1, 2))
pairs(~ O3 + temp + humidity + ibh, data = ozone)
hist(ozone$O3)
hist(ozone$temp + ozone$O3)
par(mfrow = c(1, 2))
hist(ozone$O3)
hist(ozone$temp + ozone$O3)
#install.packages("leaps")
install.packages("leaps")
install.packages("tree)
install.packages("tree")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(tree)
pruned.tree = prune.misclass(classif.tree, best=10)
install.packages("randomForest")
install.packages(gbm)
install.packages("gbm")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(tree)
library(randomForest)
library(gbm)
M.grid = c(1, 5, 10, 50, 60, 70, 80, 100, 200, 300, 400, 500, 600, 700, 800, 1000)
train_errors = rep(0, length(M.grid))
test_errors = rep(0, length(M.grid))
crabs$SP = ifelse(crabs$sp=="B", 1, 0)
for (i in 1:length(M.grid)) {
M = M.grid[i]
adaBoost.crabs = gbm(SP ~ BD + CW + CL + RW + FL + sex, data=crabs[train_id,], distribution = "adaboost", n.trees = M)
test.prob = predict(adaBoost.crabs, newdata=crabs[-train_id,], n.trees=M, type='response')
test.pred = ifelse(test.prob >= 0.5, 1, -1)
test.confusion_mat = table(test.pred, crabs$SP[-train_id])
test_errors[i] = (test.confusion_mat[1,2] + test.confusion_mat[2,1])/40
train.prob = predict(adaBoost.crabs, newdata=crabs[train_id,], n.trees=M, type='response')
train.pred = ifelse(train.prob >= 0.5, 1, -1)
train.confusion_mat = table(train.pred, crabs$SP[train_id])
train_errors[i] = (train.confusion_mat[1,2] + train.confusion_mat[2,1])/160
}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(tree)
library(randomForest)
library(gbm)
data("crabs")
set.seed(6789)
blueMale = which(crabs$sp == "B" & crabs$sex == "M")
orangeMale = which(crabs$sp == "O" & crabs$sex == "M")
blueFemale = which(crabs$sp == "B" & crabs$sex == "F")
orangeFemale = which(crabs$sp == "O" & crabs$sex == "F")
train_id = c(sample(blueMale, size = trunc(0.80 * length(blueMale))),
sample(orangeMale, size = trunc(0.80 * length(orangeMale))),
sample(blueFemale, size = trunc(0.80 * length(blueFemale))),
sample(orangeFemale, size = trunc(0.80 * length(orangeFemale))))
classif.tree = tree(sp ~ BD + CW + CL + RW + FL + sex, crabs[train_id,])
cv = cv.tree(classif.tree, FUN = prune.misclass)
print(cv)
par(mfrow=c(1,2))
plot(cv$size, cv$dev / length(train_id), ylab="cv error", xlab="size",type="b")
plot(cv$k, cv$dev / length(train_id), ylab="cv error", xlab="k",type="b")
pruned.tree = prune.misclass(classif.tree, best=10)
plot(pruned.tree)
text(pruned.tree, pretty=0)
train.pred = predict(pruned.tree, crabs[train_id,],type="class")
table(train.pred, crabs$sp[train_id])
(4+4)/length(train_id)
test.pred = predict(pruned.tree, crabs[-train_id,],type="class")
table(test.pred, crabs$sp[-train_id])
(4+3)/(200-length(train_id))
rf = randomForest(sp ~ BD + CW + CL + RW + FL + sex, data=crabs,
subset=train_id, mtry=5, importance=TRUE, ntree=1000)
rf
varImpPlot(rf)
train.pred = predict(rf, crabs[train_id,],type="class")
table(train.pred, crabs$sp[train_id])
(0+0)/length(train_id)
test.pred = predict(rf, crabs[-train_id,],type="class")
table(test.pred, crabs$sp[-train_id])
(1+2)/(200-length(train_id))
M.grid = c(1, 5, 10, 50, 60, 70, 80, 100, 200, 300, 400, 500, 600, 700, 800, 1000)
train_errors = rep(0, length(M.grid))
test_errors = rep(0, length(M.grid))
crabs$SP = ifelse(crabs$sp=="B", 1, 0)
for (i in 1:length(M.grid)) {
M = M.grid[i]
adaBoost.crabs = gbm(SP ~ BD + CW + CL + RW + FL + sex, data=crabs[train_id,], distribution = "adaboost", n.trees = M)
test.prob = predict(adaBoost.crabs, newdata=crabs[-train_id,], n.trees=M, type='response')
test.pred = ifelse(test.prob >= 0.5, 1, -1)
test.confusion_mat = table(test.pred, crabs$SP[-train_id])
test_errors[i] = (test.confusion_mat[1,2] + test.confusion_mat[2,1])/40
train.prob = predict(adaBoost.crabs, newdata=crabs[train_id,], n.trees=M, type='response')
train.pred = ifelse(train.prob >= 0.5, 1, -1)
train.confusion_mat = table(train.pred, crabs$SP[train_id])
train_errors[i] = (train.confusion_mat[1,2] + train.confusion_mat[2,1])/160
}
errors = data.frame("M"=M.grid, "training.error"=train_errors, "test.error"=test_errors)
plot(errors$training.error ~ log(errors$M), type = 'b', col = 'green',
xlab = "log(M)", ylab = "Classification error", ylim = c(0, 0.6))
lines(errors$test.error ~ log(errors$M), type = 'b', col = 'blue')
legend("topright", c("Training error", "Test error"), col = c("green", "blue"), lwd=1)
M.grid[which.min(train_errors)]
M.grid
view(M.grid)
View(adaBoost.crabs)
View(adaBoost.crabs)
View(M.grid)
M.grid = c(1, 5, 10, 50, 60, 70, 80, 100, 200, 300, 400, 500, 600, 700, 800, 1000)
train_errors = rep(0, length(M.grid))
test_errors = rep(0, length(M.grid))
crabs$SP = ifelse(crabs$sp=="B", 1, 0)
for (i in 1:length(M.grid)) {
M = M.grid[i]
adaBoost.crabs = gbm(SP ~ BD + CW + CL + RW + FL + sex, data=crabs[train_id,], distribution = "adaboost", n.trees = M)
test.prob = predict(adaBoost.crabs, newdata=crabs[-train_id,], n.trees=M, type='response')
test.pred = ifelse(test.prob >= 0.5, 1, -1)
test.confusion_mat = table(test.pred, crabs$SP[-train_id])
test_errors[i] = (test.confusion_mat[1,2] + test.confusion_mat[2,1])/40
train.prob = predict(adaBoost.crabs, newdata=crabs[train_id,], n.trees=M, type='response')
train.pred = ifelse(train.prob >= 0.5, 1, -1)
train.confusion_mat = table(train.pred, crabs$SP[train_id])
train_errors[i] = (train.confusion_mat[1,2] + train.confusion_mat[2,1])/160
}
errors = data.frame("M"=M.grid, "training.error"=train_errors, "test.error"=test_errors)
plot(errors$training.error ~ log(errors$M), type = 'b', col = 'green',
xlab = "log(M)", ylab = "Classification error", ylim = c(0, 0.6))
lines(errors$test.error ~ log(errors$M), type = 'b', col = 'blue')
legend("topright", c("Training error", "Test error"), col = c("green", "blue"), lwd=1)
M.grid[which.min(train_errors)]
M.grid = c(1, 5, 10, 50, 60, 70, 80, 100, 200, 300, 400, 500, 600, 700, 800, 1000)
train_errors = rep(0, length(M.grid))
test_errors = rep(0, length(M.grid))
crabs$SP = ifelse(crabs$sp=="B", 1, 0)
for (i in 1:length(M.grid)) {
M = M.grid[i]
adaBoost.crabs = gbm(SP ~ BD + CW + CL + RW + FL + sex, data=crabs[train_id,], distribution = "adaboost", n.trees = M)
test.prob = predict(adaBoost.crabs, newdata=crabs[-train_id,], n.trees=M, type='response')
test.pred = ifelse(test.prob >= 0.5, 1, -1)
test.confusion_mat = table(test.pred, crabs$SP[-train_id])
test_errors[i] = (test.confusion_mat[1,2] + test.confusion_mat[2,1])/40
train.prob = predict(adaBoost.crabs, newdata=crabs[train_id,], n.trees=M, type='response')
train.pred = ifelse(train.prob >= 0.5, 1, -1)
train.confusion_mat = table(train.pred, crabs$SP[train_id])
train_errors[i] = (train.confusion_mat[1,2] + train.confusion_mat[2,1])/160
}
errors = data.frame("M"=M.grid, "training.error"=train_errors, "test.error"=test_errors)
plot(errors$training.error ~ log(errors$M), type = 'b', col = 'green',
xlab = "log(M)", ylab = "Classification error", ylim = c(0, 0.6))
lines(errors$test.error ~ log(errors$M), type = 'b', col = 'blue')
legend("topright", c("Training error", "Test error"), col = c("green", "blue"), lwd=1)
M.grid[which.min(train_errors)]
test_errors[700]
train_errors[700]
M.grid
M.grid = c(1, 5, 10, 50, 60, 70, 80, 100, 200, 300, 400, 500, 600, 700, 800, 1000)
train_errors = rep(0, length(M.grid))
test_errors = rep(0, length(M.grid))
crabs$SP = ifelse(crabs$sp=="B", 1, 0)
for (i in 1:length(M.grid)) {
M = M.grid[i]
adaBoost.crabs = gbm(SP ~ BD + CW + CL + RW + FL + sex, data=crabs[train_id,], distribution = "adaboost", n.trees = M)
test.prob = predict(adaBoost.crabs, newdata=crabs[-train_id,], n.trees=M, type='response')
test.pred = ifelse(test.prob >= 0.5, 1, -1)
test.confusion_mat = table(test.pred, crabs$SP[-train_id])
test_errors[i] = (test.confusion_mat[1,2] + test.confusion_mat[2,1])/40
train.prob = predict(adaBoost.crabs, newdata=crabs[train_id,], n.trees=M, type='response')
train.pred = ifelse(train.prob >= 0.5, 1, -1)
train.confusion_mat = table(train.pred, crabs$SP[train_id])
train_errors[i] = (train.confusion_mat[1,2] + train.confusion_mat[2,1])/160
}
errors = data.frame("M"=M.grid, "training.error"=train_errors, "test.error"=test_errors)
plot(errors$training.error ~ log(errors$M), type = 'b', col = 'green',
xlab = "log(M)", ylab = "Classification error", ylim = c(0, 0.6))
lines(errors$test.error ~ log(errors$M), type = 'b', col = 'blue')
legend("topright", c("Training error", "Test error"), col = c("green", "blue"), lwd=1)
M.grid[which.min(train_errors)]
test_errors[14]
train_errors[14]
knitr::opts_chunk$set(echo = TRUE)
knitr::include_graphics("#3.jpg")
knitr::include_graphics("ISLR #2.jpg")
knitr::include_graphics("ISLR #6.jpg")
library(MASS)
library(tree)
library(randomForest)
library(gbm)
knitr::include_graphics("ISLR #2.jpg")
install.packages("haven")
install.packages("splkine")
install.packages("spline")
install.packages("splines")
install.packages("e1071")
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
set.seed(1)
n_cost = 20
costvec = 10^seq(-3, -1, length.out = n_cost)
tuned = tune(svm, sp ~ FL + RW + CL + CW + BD, data = crabs[train_id,], kernel = "linear", scale = F, ranges = list(cost = costvec))
library(e1071)
set.seed(1)
n_cost = 20
costvec = 10^seq(-3, -1, length.out = n_cost)
tuned = tune(svm, sp ~ FL + RW + CL + CW + BD, data = crabs[train_id,], kernel = "linear", scale = F, ranges = list(cost = costvec))
View(fertility)
library(leaps)
data(fat)
library(faraway)
data(fat)
pairs(fat)
summary(fat)
attach(fat)
hist(siri)
hist(density)
hist(age)
hist(weight)
hist(height)
hist(adipos)
hist(abdom)
hist(siri)
hist(density)
hist(age)
hist(weight)
hist(height)
hist(adipos)
hist(abdom)
summary(fat)
tinytex::reinstall_tinytex()
train_data = train_data[,seq(2, 145)]
rm(list=ls())
# Set working directory. Just hit enter if you're already on it
getwd()
wd = readline(prompt="What is your final project directory?")
setwd(wd)
getwd()
}
train_data = read.csv("train_data.csv")
test_data = read.csv("test_data.csv")
train_data = train_data[,seq(2, 145)]
test_data = test_data[,seq(2, 145)]
rm(wd)
rm(list=ls())
# Set working directory. Just hit enter if you're already on it
getwd()
wd = readline(prompt="What is your final project directory?")
if (nchar(wd) > 0){
setwd(wd)
getwd()
}
train_data = read.csv("train_data.csv")
test_data = read.csv("test_data.csv")
train_data = train_data[,seq(2, 145)]
test_data = test_data[,seq(2, 145)]
rm(wd)
rm(list=ls())
# Set working directory. Just hit enter if you're already on it
getwd()
wd = readline(prompt="What is your final project directory?")
setwd(wd)
getwd()
}
train_data = read.csv("train_data.csv")
test_data = read.csv("test_data.csv")
train_data = train_data[,seq(2, 145)]
test_data = test_data[,seq(2, 145)]
set.seed(1)
library(glmnet)
MSE <- function(y1, y2){
mean((y1-y2)^2)
}
train_data <- read.csv("./train_data.csv")
train_data <- subset(train_data, select = -c(y))
test_data <- read.csv("./test_data.csv")
test_data <- subset(test_data, select = names(train_data))
head(train_data)
lm.fit <- lm(LBXTC ~ DR1TCHOL, train_data)
summary(lm.fit)
dim(train_data)
dim(test_data)
train_pred <- predict(lm.fit, train_data)
MSE(train_data$LBXTC, train_pred)
test_pred <- predict(lm.fit, test_data)
MSE(test_data$LBXTC, test_pred)
fit <- lm(LBXTC ~ poly(DR1TCHOL, 10), data = train_data)
summary(fit)
fit <- lm(LBXTC ~ poly(DR1TCHOL, 4), data = train_data)
summary(fit)
train_pred <- predict(fit, train_data)
MSE(train_data$LBXTC, train_pred)
test_pred <- predict(fit, test_data)
MSE(test_data$LBXTC, test_pred)
diet_train <- subset(train_data, select = -c(SEQN, SMAQUEX2,
LBDTCSI, WTDRD1, WTDR2D, DR1EXMER, DRDINT, DR1DBIH,
DR1LANG, DR1TNUMF, DR1DAY, DBQ095Z, DRQSPREP, DRQSDIET,
DR1_300, DR1TWS, SDDSRVYR, RIDEXMON, RIAGENDR, RIDAGEYR,
RIDRETH1, DMDCITZN, DMDHHSIZ, DMDFMSIZ, INDHHIN2,
INDFMIN2, INDFMPIR, DMDHRGND, DMDHRAGE, DMDHREDU,
DMDHRMAR, SIALANG, SIAPROXY, SIAINTRP, FIALANG,
MIALANG, FIAPROXY, MIAPROXY, FIAINTRP, MIAINTRP,
WTINT2YR, WTMEC2YR, SDMVPSU, DR1BWATZ, 1))
diet_test <- subset(test_data, select = names(diet_train))
names(diet_train)
names(diet_test)
head(diet_train)
full <- lm(LBXTC ~ ., data = diet_train)
summary(full)
train_pred <- predict(full, train_data)
MSE(train_data$LBXTC, train_pred)
test_pred <- predict(full, test_data)
MSE(test_data$LBXTC, test_pred)
y_train <- diet_train$LBXTC
x_train <- as.matrix(subset(diet_train, select = -c(LBXTC)))
y_test <- diet_test$LBXTC
x_test <- as.matrix(subset(diet_test, select = -c(LBXTC)))
library(glmnet)
lasso <- cv.glmnet(x_train, y_train, alpha = 1)
bestl <- lasso$lambda.min
fit.lass <- glmnet(x_train, y_train, alpha = 1,
lambda = bestl)
fit.lass$beta
lasso_test <- predict(fit.lass, s = bestl,
newx = x_test)
lasso_train <- predict(fit.lass, s = bestl,
newx = x_train)
err_train <- mean((train_data$LBXTC - lasso_train)^2)
err_train
err_test <- mean((test_data$LBXTC - lasso_test)^2)
err_test
hist(train_data$BPXDI1)
hist(train_data$BPXDI2)
hist(train_data$BPXDI3)
hist(train_data$BPXSY1)
hist(train_data$BPXSY2)
hist(train_data$BPXSY3)
pairs(~ BPXSY1 + BPXSY2 + BPXSY3 + BPXDI1 + BPXDI2 + BPXDI3, data = train_data)
hist(train_data$LBXTC)
hist(train_data$LBDTCSI)
RIDRETH1
train_data$RIDRETH1
train_data$RIDRETH1
hist(train_data$RIDRETH1)
View(test_data)
View(test_data)
pairs(~ DR1TKCAL + DR1TPROT + DR1TCARB + DR1TSUGR + DR1TFIBE + DR1TTFAT + DR1TSFAT + DRQTMFAT + DRQTPFAT, data = train_data)
pairs(~ DR1TKCAL + DR1TPROT + DR1TCARB + DR1TSUGR + DR1TFIBE + DR1TTFAT + DR1TSFAT + DR1TMFAT + DR1TPFAT, data = train_data)
pairs(~ DR1TKCAL + DR1TPROT + DR1TCARB + DR1TSUGR + DR1TFIBE, data = train_data)
pairs(~ DR1TTFAT + DR1TSFAT + DR1TMFAT + DR1TPFAT, data = train_data)
