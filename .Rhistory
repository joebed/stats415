train.confusion_mat = table(train.pred, crabs$SP[train_id])
train_errors[i] = (train.confusion_mat[1,2] + train.confusion_mat[2,1])/160
}
errors = data.frame("M"=M.grid, "training.error"=train_errors, "test.error"=test_errors)
plot(errors$training.error ~ log(errors$M), type = 'b', col = 'green',
xlab = "log(M)", ylab = "Classification error", ylim = c(0, 0.6))
lines(errors$test.error ~ log(errors$M), type = 'b', col = 'blue')
legend("topright", c("Training error", "Test error"), col = c("green", "blue"), lwd=1)
M.grid[which.min(train_errors)]
M.grid
view(M.grid)
View(adaBoost.crabs)
View(adaBoost.crabs)
View(M.grid)
M.grid = c(1, 5, 10, 50, 60, 70, 80, 100, 200, 300, 400, 500, 600, 700, 800, 1000)
train_errors = rep(0, length(M.grid))
test_errors = rep(0, length(M.grid))
crabs$SP = ifelse(crabs$sp=="B", 1, 0)
for (i in 1:length(M.grid)) {
M = M.grid[i]
adaBoost.crabs = gbm(SP ~ BD + CW + CL + RW + FL + sex, data=crabs[train_id,], distribution = "adaboost", n.trees = M)
test.prob = predict(adaBoost.crabs, newdata=crabs[-train_id,], n.trees=M, type='response')
test.pred = ifelse(test.prob >= 0.5, 1, -1)
test.confusion_mat = table(test.pred, crabs$SP[-train_id])
test_errors[i] = (test.confusion_mat[1,2] + test.confusion_mat[2,1])/40
train.prob = predict(adaBoost.crabs, newdata=crabs[train_id,], n.trees=M, type='response')
train.pred = ifelse(train.prob >= 0.5, 1, -1)
train.confusion_mat = table(train.pred, crabs$SP[train_id])
train_errors[i] = (train.confusion_mat[1,2] + train.confusion_mat[2,1])/160
}
errors = data.frame("M"=M.grid, "training.error"=train_errors, "test.error"=test_errors)
plot(errors$training.error ~ log(errors$M), type = 'b', col = 'green',
xlab = "log(M)", ylab = "Classification error", ylim = c(0, 0.6))
lines(errors$test.error ~ log(errors$M), type = 'b', col = 'blue')
legend("topright", c("Training error", "Test error"), col = c("green", "blue"), lwd=1)
M.grid[which.min(train_errors)]
M.grid = c(1, 5, 10, 50, 60, 70, 80, 100, 200, 300, 400, 500, 600, 700, 800, 1000)
train_errors = rep(0, length(M.grid))
test_errors = rep(0, length(M.grid))
crabs$SP = ifelse(crabs$sp=="B", 1, 0)
for (i in 1:length(M.grid)) {
M = M.grid[i]
adaBoost.crabs = gbm(SP ~ BD + CW + CL + RW + FL + sex, data=crabs[train_id,], distribution = "adaboost", n.trees = M)
test.prob = predict(adaBoost.crabs, newdata=crabs[-train_id,], n.trees=M, type='response')
test.pred = ifelse(test.prob >= 0.5, 1, -1)
test.confusion_mat = table(test.pred, crabs$SP[-train_id])
test_errors[i] = (test.confusion_mat[1,2] + test.confusion_mat[2,1])/40
train.prob = predict(adaBoost.crabs, newdata=crabs[train_id,], n.trees=M, type='response')
train.pred = ifelse(train.prob >= 0.5, 1, -1)
train.confusion_mat = table(train.pred, crabs$SP[train_id])
train_errors[i] = (train.confusion_mat[1,2] + train.confusion_mat[2,1])/160
}
errors = data.frame("M"=M.grid, "training.error"=train_errors, "test.error"=test_errors)
plot(errors$training.error ~ log(errors$M), type = 'b', col = 'green',
xlab = "log(M)", ylab = "Classification error", ylim = c(0, 0.6))
lines(errors$test.error ~ log(errors$M), type = 'b', col = 'blue')
legend("topright", c("Training error", "Test error"), col = c("green", "blue"), lwd=1)
M.grid[which.min(train_errors)]
test_errors[700]
train_errors[700]
M.grid
M.grid = c(1, 5, 10, 50, 60, 70, 80, 100, 200, 300, 400, 500, 600, 700, 800, 1000)
train_errors = rep(0, length(M.grid))
test_errors = rep(0, length(M.grid))
crabs$SP = ifelse(crabs$sp=="B", 1, 0)
for (i in 1:length(M.grid)) {
M = M.grid[i]
adaBoost.crabs = gbm(SP ~ BD + CW + CL + RW + FL + sex, data=crabs[train_id,], distribution = "adaboost", n.trees = M)
test.prob = predict(adaBoost.crabs, newdata=crabs[-train_id,], n.trees=M, type='response')
test.pred = ifelse(test.prob >= 0.5, 1, -1)
test.confusion_mat = table(test.pred, crabs$SP[-train_id])
test_errors[i] = (test.confusion_mat[1,2] + test.confusion_mat[2,1])/40
train.prob = predict(adaBoost.crabs, newdata=crabs[train_id,], n.trees=M, type='response')
train.pred = ifelse(train.prob >= 0.5, 1, -1)
train.confusion_mat = table(train.pred, crabs$SP[train_id])
train_errors[i] = (train.confusion_mat[1,2] + train.confusion_mat[2,1])/160
}
errors = data.frame("M"=M.grid, "training.error"=train_errors, "test.error"=test_errors)
plot(errors$training.error ~ log(errors$M), type = 'b', col = 'green',
xlab = "log(M)", ylab = "Classification error", ylim = c(0, 0.6))
lines(errors$test.error ~ log(errors$M), type = 'b', col = 'blue')
legend("topright", c("Training error", "Test error"), col = c("green", "blue"), lwd=1)
M.grid[which.min(train_errors)]
test_errors[14]
train_errors[14]
knitr::opts_chunk$set(echo = TRUE)
knitr::include_graphics("#3.jpg")
knitr::include_graphics("ISLR #2.jpg")
knitr::include_graphics("ISLR #6.jpg")
library(MASS)
library(tree)
library(randomForest)
library(gbm)
knitr::include_graphics("ISLR #2.jpg")
install.packages("haven")
install.packages("splkine")
install.packages("spline")
install.packages("splines")
install.packages("e1071")
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
set.seed(1)
n_cost = 20
costvec = 10^seq(-3, -1, length.out = n_cost)
tuned = tune(svm, sp ~ FL + RW + CL + CW + BD, data = crabs[train_id,], kernel = "linear", scale = F, ranges = list(cost = costvec))
library(e1071)
set.seed(1)
n_cost = 20
costvec = 10^seq(-3, -1, length.out = n_cost)
tuned = tune(svm, sp ~ FL + RW + CL + CW + BD, data = crabs[train_id,], kernel = "linear", scale = F, ranges = list(cost = costvec))
View(fertility)
library(leaps)
data(fat)
library(faraway)
data(fat)
pairs(fat)
summary(fat)
attach(fat)
hist(siri)
hist(density)
hist(age)
hist(weight)
hist(height)
hist(adipos)
hist(abdom)
hist(siri)
hist(density)
hist(age)
hist(weight)
hist(height)
hist(adipos)
hist(abdom)
summary(fat)
tinytex::reinstall_tinytex()
train_data = train_data[,seq(2, 145)]
rm(list=ls())
# Set working directory. Just hit enter if you're already on it
getwd()
wd = readline(prompt="What is your final project directory?")
setwd(wd)
getwd()
}
train_data = read.csv("train_data.csv")
test_data = read.csv("test_data.csv")
train_data = train_data[,seq(2, 145)]
test_data = test_data[,seq(2, 145)]
rm(wd)
rm(list=ls())
# Set working directory. Just hit enter if you're already on it
getwd()
wd = readline(prompt="What is your final project directory?")
if (nchar(wd) > 0){
setwd(wd)
getwd()
}
train_data = read.csv("train_data.csv")
test_data = read.csv("test_data.csv")
train_data = train_data[,seq(2, 145)]
test_data = test_data[,seq(2, 145)]
rm(wd)
rm(list=ls())
# Set working directory. Just hit enter if you're already on it
getwd()
wd = readline(prompt="What is your final project directory?")
setwd(wd)
getwd()
}
train_data = read.csv("train_data.csv")
test_data = read.csv("test_data.csv")
train_data = train_data[,seq(2, 145)]
test_data = test_data[,seq(2, 145)]
set.seed(1)
library(glmnet)
MSE <- function(y1, y2){
mean((y1-y2)^2)
}
train_data <- read.csv("./train_data.csv")
train_data <- subset(train_data, select = -c(y))
test_data <- read.csv("./test_data.csv")
test_data <- subset(test_data, select = names(train_data))
head(train_data)
lm.fit <- lm(LBXTC ~ DR1TCHOL, train_data)
summary(lm.fit)
dim(train_data)
dim(test_data)
train_pred <- predict(lm.fit, train_data)
MSE(train_data$LBXTC, train_pred)
test_pred <- predict(lm.fit, test_data)
MSE(test_data$LBXTC, test_pred)
fit <- lm(LBXTC ~ poly(DR1TCHOL, 10), data = train_data)
summary(fit)
fit <- lm(LBXTC ~ poly(DR1TCHOL, 4), data = train_data)
summary(fit)
<<<<<<< HEAD
plot(lm.fit)
plot(lm.fit)
plot(fit)
fit <- lm(LBXTC ~ log(DR1TCHOL), data = train_data)
unnecessary <- c(SMAQUEX2, LBDTCSI, WTDRD1, WTDR2D, DR1EXMER,
DRDINT, DR1DBIH, DR1LANG, DR1NUMF, DR1DAY,
DBQ095Z, DRQSPREP, DRQSDIET)
unnecessary <- c(train_data$SMAQUEX2, train_data$LBDTCSI, train_data$WTDRD1, train_data$WTDR2D, train_data$DR1EXMER, train_data$DRDINT, train_data$DR1DBIH, train_data$DR1LANG, DR1NUMF, DR1DAY,
DBQ095Z, DRQSPREP, DRQSDIET)
unnecessary <- c(train_data$SMAQUEX2, train_data$LBDTCSI, train_data$WTDRD1, train_data$WTDR2D, train_data$DR1EXMER, train_data$DRDINT, train_data$DR1DBIH, train_data$DR1LANG, train_data$DR1NUMF, train_data$DR1DAY, train_data$DBQ095Z, train_data$DRQSPREP, train_data$DRQSDIET)
fit <- lm(LBXTC ~ . -unnecessary, train_data)
all_var <- subset(train_data, select = -c(SMAQUEX2, LBDTCSI, WTDRD1, WTDR2D, DR1EXMER, DRDINT, DR1DBIH, DR1LANG, DR1NUMF, DR1DAY, DBQ095Z, DRQSPREP, DRQSDIET))
all_var <- subset(train_data, select = -c(SMAQUEX2, LBDTCSI, WTDRD1, WTDR2D, DR1EXMER, DRDINT, DR1DBIH, DR1LANG, DR1TNUMF, DR1DAY, DBQ095Z, DRQSPREP, DRQSDIET))
fit <- lm(LBXTC ~ ., all_var)
summary(fit)
all_var <- subset(train_data, select = -c(y, SEQN, SMAQUEX2, LBDTCSI, WTDRD1, WTDR2D, DR1EXMER, DRDINT, DR1DBIH, DR1LANG, DR1TNUMF, DR1DAY, DBQ095Z, DRQSPREP, DRQSDIET))
library(leaps)
regfit.full <- regsubsets(LBXTC ~ ., all_var)
summary(regfit.full)
regfit.full <- regsubsets(LBXTC ~ ., all_var)
View(all_var)
View(train_data)
source('~/R/Stats_415/Stats_415_Final_Project/set_workspace.R', echo=TRUE)
setwd("~/R/Stats_415/Stats_415_Final_Project")
train_data = read.csv("train_data.csv")
test_data = read.csv("test_data.csv")
train_data = train_data[,seq(2, 145)]
test_data = test_data[,seq(2, 145)]
rm(wd)
str(train_data)
head(train_data)
summary(train_data$SMAQUEX2)
table(train_data$SMAQUEX2)
table(train_data$SMAQUEX2)
LBXTC
summary(train_data$LBXTC)
str(train_data)
head(train_data)
train_data$BP_Systolic = mean(train_data$BPXSY1, train_data$BPXSY2, train_data$BPXSY3)
train_data$BP_Systolic = (train_data$BPXSY1 + train_data$BPXSY2 + train_data$BPXSY3)/3
summary(train_data$BP_Systolic)
train_data$BP_Systolic
summary(train_data$BPXSY1)
summary(train_data$BP_Systolic)
train_data$BP_Systolic = (train_data$BPXDI1 + train_data$BPXDI2 + train_data$BPXDI3)/3
train_data$BP_Systolic = (train_data$BPXSY1 + train_data$BPXSY2 + train_data$BPXSY3)/3
train_data$BP_Diastolic = (train_data$BPXDI1 + train_data$BPXDI2 + train_data$BPXDI3)/3
train_data$BP_Diastolic = (train_data$BPXDI1 + train_data$BPXDI2 + train_data$BPXDI3)/3
summary(train_data$BP_Diastolic)
summary(train_data$LBXTC)
summary(train_data$BMXBMI)
## Decision Tree with Random Forests
table(train_data$SMAQUEX2)
train_data$smokes = as.factor(ifelse(train_data$SMAQUEX2 < 0, "No", "Yes")
train_data$smokes = as.factor(ifelse(train_data$SMAQUEX2 < 0, "No", "Yes"))
train_data$smokes = as.factor(ifelse(train_data$SMAQUEX2 < 0, "No", "Yes"))
summary(train_data$smokes)
train_data$smokes = NULL
## Decision Tree with Boosting
cholest = read.csv("train_data.csv")
cholest$LBXTC
summary(cholest$LBXTC)
?unscale
source('~/R/Stats_415/Stats_415_Final_Project/data/set_up_data.R', echo=TRUE)
train_data = read.csv("train_data.csv")
source('~/R/Stats_415/Stats_415_Final_Project/data/set_up_data.R', echo=TRUE)
summary(train_data$LBXTC)
train_data$highchol = as.factor(ifelse(train_data$LBXTC >= 239, 1, 0))
train_data$borderlinechol = as.factor(ifelse(train_data$LBXTC >= 200, 1, 0))
test_data$borderlinechol = as.factor(ifelse(test_data$LBXTC >= 200, 1, 0))
test_data$borderlinechol = as.factor(ifelse(test_data$LBXTC >= 200, 1, 0))
train_data = scale(train_data)
test_data = scale(test_data)
train_data[,] = scale(train_data[,])
lm(highchol~!borderlinechol, data = train_data)
#install.packages("randomForest")
library(randomForest)
train_data$y = NULL
#install.packages("randomForest")
library(randomForest)
train_data$LBXTC = NULL
test_data$LBXTC = NULL
m = 12
mod_rf = randomForest(highchol~ .-borderlinechol, data = train_data, mtry = m, ntree = 10000, importance = T)
varImpPlot(mod_rf)
mod_rf = randomForest(highchol~ .-borderlinechol, data = train_data, mtry = m, ntree = 1000, importance = T)
varImpPlot(mod_rf)
train_data$LBDTCSI = NULL
test_data$LBDTCSI = NULL
mod_rf = randomForest(highchol~ .-borderlinechol, data = train_data, mtry = m, ntree = 1000, importance = T)
varImpPlot(mod_rf)
mod_rf_2 = randomForest(borderlinechol~ .-highchol, data = train_data, mtry = m, ntree = 1000, importance = T)
train_data_b$highchol = ifelse(train_data_b$highchol == "Yes", 1, 0)
test_data_b = test_data
test_data_b$highchol = ifelse(test_data_b$highchol == "Yes", 1, 0)
## Decision Tree with Boosting
train_data_b = train_data
test_data_b$highchol
test_data_b = test_data
test_data_b$highchol
head(test_data_b)
test_data$highchol = as.factor(ifelse(test_data$LBXTC >= 239, 1, 0))
source('~/R/Stats_415/Stats_415_Final_Project/data/set_up_data.R', echo=TRUE)
source('~/R/Stats_415/Stats_415_Final_Project/Decision_Trees_ANN.R', echo=TRUE)
=======
train_pred <- predict(fit, train_data)
MSE(train_data$LBXTC, train_pred)
test_pred <- predict(fit, test_data)
MSE(test_data$LBXTC, test_pred)
diet_train <- subset(train_data, select = -c(SEQN, SMAQUEX2,
LBDTCSI, WTDRD1, WTDR2D, DR1EXMER, DRDINT, DR1DBIH,
DR1LANG, DR1TNUMF, DR1DAY, DBQ095Z, DRQSPREP, DRQSDIET,
DR1_300, DR1TWS, SDDSRVYR, RIDEXMON, RIAGENDR, RIDAGEYR,
RIDRETH1, DMDCITZN, DMDHHSIZ, DMDFMSIZ, INDHHIN2,
INDFMIN2, INDFMPIR, DMDHRGND, DMDHRAGE, DMDHREDU,
DMDHRMAR, SIALANG, SIAPROXY, SIAINTRP, FIALANG,
MIALANG, FIAPROXY, MIAPROXY, FIAINTRP, MIAINTRP,
WTINT2YR, WTMEC2YR, SDMVPSU, DR1BWATZ, 1))
diet_test <- subset(test_data, select = names(diet_train))
names(diet_train)
names(diet_test)
head(diet_train)
full <- lm(LBXTC ~ ., data = diet_train)
summary(full)
train_pred <- predict(full, train_data)
MSE(train_data$LBXTC, train_pred)
test_pred <- predict(full, test_data)
MSE(test_data$LBXTC, test_pred)
y_train <- diet_train$LBXTC
x_train <- as.matrix(subset(diet_train, select = -c(LBXTC)))
y_test <- diet_test$LBXTC
x_test <- as.matrix(subset(diet_test, select = -c(LBXTC)))
library(glmnet)
lasso <- cv.glmnet(x_train, y_train, alpha = 1)
bestl <- lasso$lambda.min
fit.lass <- glmnet(x_train, y_train, alpha = 1,
lambda = bestl)
fit.lass$beta
lasso_test <- predict(fit.lass, s = bestl,
newx = x_test)
lasso_train <- predict(fit.lass, s = bestl,
newx = x_train)
err_train <- mean((train_data$LBXTC - lasso_train)^2)
err_train
err_test <- mean((test_data$LBXTC - lasso_test)^2)
err_test
hist(train_data$BPXDI1)
hist(train_data$BPXDI2)
hist(train_data$BPXDI3)
hist(train_data$BPXSY1)
hist(train_data$BPXSY2)
hist(train_data$BPXSY3)
pairs(~ BPXSY1 + BPXSY2 + BPXSY3 + BPXDI1 + BPXDI2 + BPXDI3, data = train_data)
hist(train_data$LBXTC)
hist(train_data$LBDTCSI)
RIDRETH1
train_data$RIDRETH1
train_data$RIDRETH1
hist(train_data$RIDRETH1)
View(test_data)
View(test_data)
pairs(~ DR1TKCAL + DR1TPROT + DR1TCARB + DR1TSUGR + DR1TFIBE + DR1TTFAT + DR1TSFAT + DRQTMFAT + DRQTPFAT, data = train_data)
pairs(~ DR1TKCAL + DR1TPROT + DR1TCARB + DR1TSUGR + DR1TFIBE + DR1TTFAT + DR1TSFAT + DR1TMFAT + DR1TPFAT, data = train_data)
pairs(~ DR1TKCAL + DR1TPROT + DR1TCARB + DR1TSUGR + DR1TFIBE, data = train_data)
pairs(~ DR1TTFAT + DR1TSFAT + DR1TMFAT + DR1TPFAT, data = train_data)
>>>>>>> 47691af457fedbd5b2dde08098c62fb1071e2631
table(test_pred_adaboost)
colnames(train_data)
colnames(test_data)
summary(train_data$LBXTC)
train_data$highchol = as.factor(ifelse(train_data$LBXTC >= 239, "Yes", "No"))
test_data$highchol = as.factor(ifelse(test_data$LBXTC >= 239, "Yes", "No"))
train_data$LBXTC = NULL
test_data$LBXTC = NULL
train_data$LBDTCSI = NULL
test_data$LBDTCSI = NULL
train_data$SEQN = NULL
train_data$SMAQUEX2 = NULL
test_data$SEQN = NULL
test_data$SMAQUEX2 = NULL
train_data$RIDAGEYR = NULL
test_data$RIDAGEYR = NULL
test_data$DMDHRBR4 = NULL
train_data$y = NULL
#install.packages("groupdata2")
library(groupdata2)
train_data = upsample(train_data, cat_col = "highchol")
#install.packages("randomForest")
library(randomForest)
m = 12
summary(train_data$highchol)
head(train_data)
# Set up workspace for the final project
rm(list=ls())
# Set working directory. Just hit enter if you're already on it
getwd()
wd = readline(prompt="What is your final project directory?")
if (nchar(wd) > 0){
setwd(wd)
getwd()
}
train_data = read.csv("./data/train_data.csv")
test_data = read.csv("./data/test_data.csv")
boost.df = read.csv("./data/boostdf.csv")
bart_coefs = read.csv("./data/bart_coef.csv")
train_data = train_data[,seq(2, 145)]
test_data = test_data[,seq(2, 145)]
rm(wd)
colnames(train_data)
colnames(test_data)
summary(train_data$LBXTC)
train_data$highchol = as.factor(ifelse(train_data$LBXTC >= 239, "Yes", "No"))
test_data$highchol = as.factor(ifelse(test_data$LBXTC >= 239, "Yes", "No"))
train_data$LBXTC = NULL
test_data$LBXTC = NULL
train_data$LBDTCSI = NULL
test_data$LBDTCSI = NULL
train_data$SEQN = NULL
train_data$SMAQUEX2 = NULL
test_data$SEQN = NULL
test_data$SMAQUEX2 = NULL
train_data$RIDAGEYR = NULL
test_data$RIDAGEYR = NULL
test_data$DMDHRBR4 = NULL
train_data$y = NULL
train_data = upsample(train_data, cat_col = "highchol")
#install.packages("randomForest")
library(randomForest)
m = 12
#.-borderlinechol
mod_rf = randomForest(highchol~., data = train_data, mtry = m, ntree = 1000, importance = T)
mod_rf
summary(train_data$highchol)
# In order to use this, use setwd({final_project_directory})
# getwd() gets your current working directory if you're unsure where you are at
# Clean workspace beforehand
rm(list=ls())
# Data of all years has these names
files = c("/TCHOL.XPT", "/SMQ.XPT", "/DR1TOT.XPT", "/DEMO.XPT", "/BPX.XPT", "/BMX.XPT")
library(haven)
library(dplyr)
years_data = list()
# Iterate through all years
for(i in seq(1, 5)){
# Set year
year = 2 * i + 2007
# Set string of directory, make sure to setwd to your final project directory
dir = paste("data/", year, sep="")
year_data = data.frame()
# Iterate through all files and left join
for(j in seq(1,6)){
d = paste(dir, files[j], sep="")
val = read_xpt(d)
if(j == 1){
year_data = val
}
else{
year_data = merge(year_data, val, all.x = TRUE, by = "SEQN")
}
}
if(i == 1){
years_data = year_data
}
else{
years_data = bind_rows(years_data, year_data)
}
}
# Set train and test data
train_data = read.csv("data/train.csv")
test_data = read.csv("data/test.csv")
train_data = merge(train_data, years_data, all.x = TRUE, by="SEQN")
test_data = merge(test_data, years_data, all.x = TRUE, by="SEQN")
# Remove any column where there is an NA
train_data = train_data[, colSums(is.na(train_data)) == 0]
test_data = test_data[, colSums(is.na(test_data)) == 0]
# Remove columns as outlined in group guidelines
drops = c("SMDUPCA", "SMD100BR", "DR1DRSTZ", "DRABF", "RIDSTATR")
train_data = train_data[, !(names(train_data) %in% drops)]
test_data = test_data[, !(names(test_data) %in% drops)]
# Standardize data
#train_data[seq(3, 144)] = scale(train_data[seq(3, 144)])
#test_data[seq(2, 144)] = scale(test_data[seq(2, 144)])
# Clean data
rm(list = c("d", "dir", "files", "i", "j", "year", "year_data", "years_data", "drops", "val"))
colnames(train_data)
colnames(test_data)
summary(train_data$LBXTC)
train_data$highchol = as.factor(ifelse(train_data$LBXTC >= 239, "Yes", "No"))
test_data$highchol = as.factor(ifelse(test_data$LBXTC >= 239, "Yes", "No"))
train_data$LBXTC = NULL
test_data$LBXTC = NULL
train_data$LBDTCSI = NULL
test_data$LBDTCSI = NULL
train_data$SEQN = NULL
train_data$SMAQUEX2 = NULL
test_data$SEQN = NULL
test_data$SMAQUEX2 = NULL
train_data$RIDAGEYR = NULL
test_data$RIDAGEYR = NULL
test_data$DMDHRBR4 = NULL
train_data$y = NULL
#install.packages("groupdata2")
library(groupdata2)
train_data = upsample(train_data, cat_col = "highchol")
#install.packages("randomForest")
library(randomForest)
m = 12
#.-borderlinechol
mod_rf = randomForest(highchol~., data = train_data, mtry = m, ntree = 1000, importance = T)
mod_rf
summary(train_data$highchol)
ggplot(train_data) + geom_bar(aes(x = highchol))
library(ggplot2)
ggplot(train_data) + geom_bar(aes(x = highchol))
summary(train_data$highchol)
mod_rf
#.-borderlinechol
mod_rf = randomForest(highchol~., data = train_data, mtry = m, ntree = 1000, importance = T)
